{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating the model\n",
    "\n",
    "I generated calibration data in R, in code/cleaning/clean_BEA_calibration.R. I compute our exogenous variables as...\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{P_j X_{ij}}{P_i Y_i} = \\omega_{ij} \\tilde{Z}^{\\theta- 1} \\left(\\frac{P_{j}}{Q_i}\\right)^{1-\\theta} \\rightarrow \\omega_{ij} = \\frac{\\bar{X}_{ij}}{\\bar{Y}_i}, \\text{ expenditure share at base year, normalized to sum to 1} \\\\\n",
    "\\frac{W_i L_i}{P_i Y_i} = \\alpha_i Z^{\\epsilon - 1} \\left(\\frac{W_i}{P_i}\\right)^{1-\\epsilon} \\rightarrow \\alpha_i = \\frac{\\bar{L}_i}{\\bar{Y}_i}, \\text{ value-added share at base year} \\\\\n",
    "\\beta_i = \\bar{C}_i = \\bar{Y}_i - \\sum_j \\bar{X}_{ji} = \\bar{Y}_i - \\sum_j \\bar{Y}_j (1-\\alpha_j) \\omega_{ji} = \\mathbf{Y}(\\mathbf{I} - \\text{diag}(1-\\alpha)\\Omega), \\text{ normalized to sum to 1} \\\\\n",
    "\\bar{L}_i = \\alpha_i \\bar{Y}_i = \\alpha \\times (\\mathbf{I} - \\text{diag}(1-\\alpha)\\Omega)^{-1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By default, I use 2023 as the base year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# I load calibration_data from R\n",
    "ro.r['load']('../../data/cleaned/structural/calibration_data.RData')\n",
    "ro.r['load']('../../data/cleaned/code_desc_crosswalk.RData')\n",
    "\n",
    "# Convert R data frames to pandas DataFrames\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    code_desc_crosswalk = ro.conversion.rpy2py(ro.r['code_desc_crosswalk'])\n",
    "    Y_list = ro.conversion.rpy2py(ro.r['Y_list'])\n",
    "    Omega_list = ro.conversion.rpy2py(ro.r['Omega_list'])\n",
    "    alpha_list = ro.conversion.rpy2py(ro.r['alpha_list'])\n",
    "    beta_list = ro.conversion.rpy2py(ro.r['beta_list'])\n",
    "    L_list = ro.conversion.rpy2py(ro.r['L_list'])\n",
    "    industry_tfp = ro.conversion.rpy2py(ro.r['industry_TFP'])\n",
    "    elasticity_byyear = ro.conversion.rpy2py(ro.r['elasticity_byyear'])\n",
    "    elasticity_bycode = ro.conversion.rpy2py(ro.r['elasticity_byCode'])\n",
    "    cumulative_delta_logOmega = ro.conversion.rpy2py(ro.r['cumulative_delta_logOmega_wide']).to_numpy()\n",
    "    delta_logOmega_list = ro.conversion.rpy2py(ro.r['delta_logOmega_wide_list'])\n",
    "    delta_logOmega_predicted10 = ro.conversion.rpy2py(ro.r['delta_logOmega_predicted10']).to_numpy()\n",
    "\n",
    "# generate the covariance matrix of the industry TFP\n",
    "industry_tfp_annual = industry_tfp.pivot(index='Code', columns='year', values='delta_tfp_1').to_numpy()\n",
    "industry_tfp_vcov_annual = np.cov(industry_tfp_annual)\n",
    "industry_tfp_quadrennial = industry_tfp.pivot(index='Code', columns='year', values='delta_tfp_4').to_numpy()\n",
    "industry_tfp_vcov_quadrennial = np.cov(industry_tfp_quadrennial)\n",
    "\n",
    "# set off diagonal elements to zero (assume uncorrelated shocks)\n",
    "industry_tfp_vcov_annual = np.diag(np.diag(industry_tfp_vcov_annual))\n",
    "industry_tfp_vcov_quadrennial = np.diag(np.diag(industry_tfp_vcov_quadrennial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving for equilibrium\n",
    "\n",
    "I solve for two different types of equilibrium: one where labor is not allowed to reallocate across sectors, thereby exogenously fixing $L_i$, and one where labor is fully allowed to reallocate across sectors, equating sectoral wages $W_i = W_j = W$ for all $i,j$. \n",
    "\n",
    "## No reallocation\n",
    "\n",
    "An equilibrium is defined as wages $\\{W_i\\}_{i=0}^N$, prices $\\{P_i\\}_{i=0}^N$ and quantities ${Y_i}$ such that...\n",
    "\n",
    "1) Wages are equal to the marginal product of labor, i.e. $W_i = P_i \\times \\partial F_i/\\partial L_i$ for all $i$. \n",
    "2) Prices are equal to marginal cost, i.e. $P_i = MC_i$ for all $i$.\n",
    "3) All goods markets clear, i.e. $Y_i = \\sum_j X_{ij} + C_i$. \n",
    "\n",
    "In practice, wages are perfectly pinned down by $Y,P$ and the exogenous variables. So we are solving for prices $P$ and quantities $Y$ that satisfy (2) and (3). We need to write these constraints in terms of exogenous parameters and givens, which we get via the FOC of the firm and household problems. Note that the price of the consumption bundle $\\left(\\sum_j \\beta_j p_j^{1-\\sigma}\\right)^{\\frac{1}{1-\\sigma}}$ is normalized to 1. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MC_i = A_i^{\\epsilon_i - 1} (\\alpha_i w_i^{1-\\epsilon_i}+ (1-\\alpha_i)q_i^{1-\\epsilon_i})^{\\frac{1}{1-\\epsilon_i}} \\\\\n",
    "X_{ij} = y_i (1-\\alpha_i) \\omega_{ij} p_i^{\\epsilon_i} p_j^{-\\theta_i} q_i^{\\theta_i-\\epsilon_i}Z_i^{\\epsilon_i - 1} \\tilde{Z}_i^{\\theta_i - 1} \\\\\n",
    "C_i = C \\beta_i p_i^{-\\sigma} = wL \\beta_i p_i^{-\\sigma} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Full reallocation\n",
    "\n",
    "Same definition as above, but now labor is endogenously determined, and wages are equalized across sectors. I normalize the wage $W = 1$ to be the numeraire (whereas before the consumption bundle price was normalized to 1), and I redefine aggregate consumption as $C = \\sum_i \\beta_i p_i^{1-\\sigma}$. Other than that things pass as normal.\n",
    "\n",
    "Note that prices are written using lower cases, since they are defined in relation to the normalization year. \n",
    "\n",
    "## Functions\n",
    "\n",
    "I define functions...\n",
    "\n",
    "- ```xij(A, Omega, alpha, epsilon, theta, Q, Y, P)```: computes the matrix of intermediate input demand $\\{x_{ij}\\}$ given exogenous parameters and guesses for prices and quantities.\n",
    "- ```multisector_constraints_realloc(X, A, Omega, alpha, epsilon, theta, Q, Y, P, C, sigma)```: returns a vector of $P_i - MC_i$ and $Y_i - \\sum_j x_{ij} - c_i$ for all $i$ given guesses of prices and quantities in $X$; these need to go to 0 in eqm.\n",
    "- ```multisector_constraints_no_realloc(X, A, Omega, alpha, epsilon, theta, Q, Y, P, C, sigma)```: same as above, but with labor fixed.\n",
    "- ```trivial(X)```: pure place-holder, arbitrary function of guess so I can use scipy.optimize.minimize with constraints.\n",
    "- ```eqm_solver(A, Omega, alpha, epsilon, theta, Q, Y, P, C, sigma, realloc, return_L)```: solves for equilibrium prices and quantities given exogenous parameters and a boolean for whether labor is allowed to reallocate and whether to return labor.\n",
    "- ```draw_multivariate_normal(cov, num_samples, seed)```: draws from a multivariate normal with a given covariance matrix and seed.\n",
    "- ```eqm_simulator(A, Omega, alpha, epsilon, theta, Q, Y, P, C, sigma, realloc, return_L, num_samples, seed)```: solves for equilibria given exogenous parameters and TFP shocks drawn from a multivariate normal. Returns GDP and sectoral output.\n",
    "- ```CD_simulator(A, Omega, alpha, epsilon, theta, Q, Y, P, C, sigma, realloc, return_L, num_samples, seed)```: solves for equilibria but using Cobb-Douglas production functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticity exercises\n",
    "\n",
    "Exercises examining how my estimates of heterogenous elasticities, across years and across sectors, change the predicted macro effect of sectoral shocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-varying elasticity exercise \n",
    "\n",
    "Generate realistic shocks calibrated to TFP data (annual and quadrennial). Compare equilibrium GDP distributions with high/low elasticity estimates (p75, p25), along with Cobb-Douglas benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load solver functions\n",
    "from eqm_solver_functions import *\n",
    "\n",
    "# set default values (2023)\n",
    "year = '2023'\n",
    "Omega = Omega_list[year]\n",
    "alpha = alpha_list[year].flatten()\n",
    "beta = beta_list[year].flatten()\n",
    "L = L_list[year].flatten()\n",
    "\n",
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = 0.7\n",
    "\n",
    "# load elasticities\n",
    "theta_high = elasticity_bycode['p75_theta'].to_numpy()\n",
    "theta_low = elasticity_bycode['p25_theta'].to_numpy()\n",
    "\n",
    "# set initial guesses\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# get Domar weights; used for Cobb-Douglas comparison\n",
    "domar_weights = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "\n",
    "# draw shocks\n",
    "annual_shocks = draw_multivariate_normal(industry_tfp_vcov_annual, 1000, seed=1)\n",
    "quad_shocks = draw_multivariate_normal(industry_tfp_vcov_quadrennial, 1000, seed=1) # magnified shocks = business-cycle shocks\n",
    "\n",
    "# simulate GDP and sectoral output distributions\n",
    "gdp_dist_CD_annual = CD_simulator(domar_weights, annual_shocks)\n",
    "gdp_dist_high_annual, sectoral_output_high_annual = eqm_simulator(beta, Omega, alpha, epsilon, theta_high, sigma, L, P_norm, Y_norm, annual_shocks)\n",
    "gdp_dist_low_annual, sectoral_output_low_annual = eqm_simulator(beta, Omega, alpha, epsilon, theta_low, sigma, L, P_norm, Y_norm, annual_shocks)\n",
    "gdp_dist_CD_quad = CD_simulator(domar_weights, quad_shocks)\n",
    "gdp_dist_high_quad, sectoral_output_high_quad = eqm_simulator(beta, Omega, alpha, epsilon, theta_high, sigma, L, P_norm, Y_norm, quad_shocks)\n",
    "gdp_dist_low_quad, sectoral_output_low_quad = eqm_simulator(beta, Omega, alpha, epsilon, theta_low, sigma, L, P_norm, Y_norm, quad_shocks)\n",
    "\n",
    "# save results\n",
    "gdp_dist = pd.DataFrame({'gdp_dist_CD_annual': gdp_dist_CD_annual, 'gdp_dist_high_annual': gdp_dist_high_annual, 'gdp_dist_low_annual': gdp_dist_low_annual, 'gdp_dist_CD_quad': gdp_dist_CD_quad, 'gdp_dist_high_quad': gdp_dist_high_quad, 'gdp_dist_low_quad': gdp_dist_low_quad})\n",
    "gdp_dist.to_csv(\"../../data/cleaned/structural/simulated_gdp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import gdp_dist\n",
    "gdp_dist = pd.read_csv(\"../../data/cleaned/structural/simulated_gdp.csv\")\n",
    "gdp_dist = gdp_dist.dropna() # remove rows with nans\n",
    "gdp_dist_CD_annual = gdp_dist['gdp_dist_CD_annual']\n",
    "gdp_dist_high_annual = gdp_dist['gdp_dist_high_annual']\n",
    "gdp_dist_low_annual = gdp_dist['gdp_dist_low_annual']\n",
    "gdp_dist_CD_quad = gdp_dist['gdp_dist_CD_quad']\n",
    "gdp_dist_high_quad = gdp_dist['gdp_dist_high_quad']\n",
    "gdp_dist_low_quad = gdp_dist['gdp_dist_low_quad']\n",
    "\n",
    "# Calculate statistics for annual shocks\n",
    "annual_stats = {\n",
    "    'mean': [np.mean(np.log(gdp_dist_CD_annual)), np.mean(np.log(gdp_dist_high_annual)), np.mean(np.log(gdp_dist_low_annual))],\n",
    "    'sd': [np.std(np.log(gdp_dist_CD_annual)), np.std(np.log(gdp_dist_high_annual)), np.std(np.log(gdp_dist_low_annual))],\n",
    "    'skew': [skew(np.log(gdp_dist_CD_annual)), skew(np.log(gdp_dist_high_annual)), skew(np.log(gdp_dist_low_annual))]\n",
    "}\n",
    "\n",
    "# Calculate statistics for quad shocks\n",
    "quad_stats = {\n",
    "    'mean': [np.mean(np.log(gdp_dist_CD_quad)), np.mean(np.log(gdp_dist_high_quad)), np.mean(np.log(gdp_dist_low_quad))],\n",
    "    'sd': [np.std(np.log(gdp_dist_CD_quad)), np.std(np.log(gdp_dist_high_quad)), np.std(np.log(gdp_dist_low_quad))],\n",
    "    'skew': [skew(np.log(gdp_dist_CD_quad)), skew(np.log(gdp_dist_high_quad)), skew(np.log(gdp_dist_low_quad))]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "\n",
    "results_baseline = pd.DataFrame({\n",
    "    'Cobb-Douglas': [annual_stats['mean'][0], annual_stats['sd'][0], annual_stats['skew'][0]],\n",
    "    'High elasticity': [annual_stats['mean'][1], annual_stats['sd'][1], annual_stats['skew'][1]],\n",
    "    'Low elasticity': [annual_stats['mean'][2], annual_stats['sd'][2], annual_stats['skew'][2]]\n",
    "}, index=['Mean', 'SD', 'Skewness'])\n",
    "\n",
    "results_quad = pd.DataFrame({\n",
    "    'Cobb-Douglas': [quad_stats['mean'][0], quad_stats['sd'][0], quad_stats['skew'][0]],\n",
    "    'High elasticity': [quad_stats['mean'][1], quad_stats['sd'][1], quad_stats['skew'][1]],\n",
    "    'Low elasticity': [quad_stats['mean'][2], quad_stats['sd'][2], quad_stats['skew'][2]],\n",
    "}, index=['Mean', 'SD', 'Skewness'])\n",
    "\n",
    "# save table to latex \n",
    "results_baseline = results_baseline.transpose()\n",
    "results_quad = results_quad.transpose()\n",
    "results = pd.concat([results_baseline, results_quad], axis=1)\n",
    "results_tab = results.to_latex(float_format = '%.3f')\n",
    "custom_header = r\"\"\"\n",
    "\\begin{tabular}{lcccccc}\n",
    "\\toprule\n",
    "& \\multicolumn{3}{c}{Annual shocks} & \\multicolumn{3}{c}{Quadrennial shocks} \\\\\n",
    "\"\"\"\n",
    "results_tab = results_tab.replace(r\"\\begin{tabular}{lrrrrrr}\", custom_header)\n",
    "with open(\"../../tables/shock_simulation_results_combined.tex\", \"w\") as f:\n",
    "    f.write(results_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector-varying elasticity exercises\n",
    "\n",
    "Generate severe shocks to individual sectors that cut their output by 75%. Compare resulting equilibrium GDP with sector-varying elasticities versus the corresponding mean imposed uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load solver functions\n",
    "from eqm_solver_functions import *\n",
    "\n",
    "# set default values (2023)\n",
    "year = '2023'\n",
    "Omega = Omega_list[year]\n",
    "alpha = alpha_list[year].flatten()\n",
    "beta = beta_list[year].flatten()\n",
    "L = L_list[year].flatten()\n",
    "\n",
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = 0.7\n",
    "\n",
    "# load elasticities\n",
    "theta_het = elasticity_bycode['mean_theta'].to_numpy()\n",
    "theta_mean = elasticity_bycode['mean_theta'].mean() * np.ones(len(alpha))\n",
    "\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# take SD across all observations in industry_tfp_annual\n",
    "industry_tfp_sd = np.std(industry_tfp_annual)\n",
    "outlier = -np.log(2)\n",
    "extreme_shocks = []\n",
    "for i in range(len(alpha)): \n",
    "    shock = np.zeros(len(alpha))\n",
    "    shock[i] = outlier\n",
    "    extreme_shocks.append(shock)\n",
    "\n",
    "gdp_dist_het_extreme, sectoral_output_het_extreme = eqm_simulator(beta, Omega, alpha, epsilon, theta_het, sigma, L, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_dist_mean_extreme, sectoral_output_mean_extreme = eqm_simulator(beta, Omega, alpha, epsilon, theta_mean, sigma, L, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_diff = np.log(gdp_dist_het_extreme) - np.log(gdp_dist_mean_extreme) # log deviation from non-stochastic mean\n",
    "\n",
    "# dataframe with industry labels\n",
    "gdp_diff = pd.DataFrame({'Het.': np.log(gdp_dist_het_extreme), 'Uniform': np.log(gdp_dist_mean_extreme), 'Difference': gdp_diff, 'Code': elasticity_bycode['Code']})\n",
    "gdp_diff = gdp_diff.merge(code_desc_crosswalk, on='Code')\n",
    "# make table of industry description and smallest differences\n",
    "diff_tab = gdp_diff.sort_values(by='Het.') \n",
    "diff_tab = diff_tab[['Industry Description', 'Het.', 'Uniform', 'Difference']]\n",
    "diff_tab = diff_tab.head(3)\n",
    "# rename columns\n",
    "diff_tab.columns = ['Industry Description', '$\\\\Delta$ GDP (Het.)', '$\\\\Delta$ GDP (Unif.)', 'Difference']\n",
    "diff_tab.to_latex(\"../../tables/industry_diff_extreme.tex\", index=False, float_format = '%.3f', column_format='lccc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Share parameter exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share parameters and changes in sectoral output\n",
    "\n",
    "Using the cumulative sum of share parameter shifts estimated empirically, simulate counterfactual Omega corresponding to 2023 (assuming no other exogenous parameters change). Estimate response of sectoral output, with and without labor reallocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial values for 1997\n",
    "year = '1997'\n",
    "Omega = Omega_list[year]\n",
    "alpha = alpha_list[year].flatten()\n",
    "beta = beta_list[year].flatten()\n",
    "L = L_list[year].flatten()\n",
    "\n",
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = .7\n",
    "\n",
    "# load elasticities\n",
    "theta = elasticity_bycode['theta'].to_numpy()\n",
    "theta_mean = elasticity_byyear['theta'].mean() * np.ones(len(alpha))\n",
    "\n",
    "# get counterfactual Omega\n",
    "Omega_counterfactual = Omega.copy()\n",
    "delta_Omega = np.exp(cumulative_delta_logOmega)\n",
    "Omega_counterfactual = Omega_counterfactual * delta_Omega\n",
    "\n",
    "Omega_counterfactual = Omega_counterfactual / Omega_counterfactual.sum(axis=1)[:, np.newaxis] # normalize rows to sum to 1\n",
    "\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# solve for counterfactual output without reallocation\n",
    "C_0, Y_0 = eqm_solver(np.ones(len(alpha)), beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm)\n",
    "C_1, Y_1 = eqm_solver(np.ones(len(alpha)), beta, Omega_counterfactual, alpha, epsilon, theta, sigma, L, P_norm, Y_norm)\n",
    "counterfactual_output = pd.DataFrame({'Code': elasticity_bycode['Code'], 'Y_1': Y_1, 'Y_0': Y_0, 'log_change': np.log(Y_1) - np.log(Y_0)})\n",
    "counterfactual_output = counterfactual_output.sort_values(by='log_change', ascending=False)\n",
    "\n",
    "# solve for counterfactual output with reallocation\n",
    "C_0, Y_0 = eqm_solver(np.ones(len(alpha)), beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, reallocate=True)\n",
    "C_1, Y_1 = eqm_solver(np.ones(len(alpha)), beta, Omega_counterfactual, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, reallocate=True)\n",
    "counterfactual_output_realloc = pd.DataFrame({'Code': elasticity_bycode['Code'], 'Y_1': Y_1, 'Y_0': Y_0, 'log_change': np.log(Y_1) - np.log(Y_0)})\n",
    "counterfactual_output_realloc = counterfactual_output_realloc.sort_values(by='log_change', ascending=False)\n",
    "\n",
    "# select 3 largest increases in output, without and with reallocation\n",
    "counterfactual_output_combined_table = pd.concat([counterfactual_output.head(3), counterfactual_output_realloc.head(3)])\n",
    "counterfactual_output_combined_table = counterfactual_output_combined_table.merge(code_desc_crosswalk, on='Code')\n",
    "counterfactual_output_combined_table = counterfactual_output_combined_table[['Industry Description', 'log_change']]\n",
    "# abridge industry descriptions at 50 characters\n",
    "# add ellipsis if longer\n",
    "counterfactual_output_combined_table['Industry Description'] = counterfactual_output_combined_table['Industry Description'].apply(lambda x: x[:50] + '...' if len(x) > 50 else x)\n",
    "counterfactual_output_combined_table = counterfactual_output_combined_table.rename(columns={'log_change': 'Log Change in Output'})\n",
    "\n",
    "latex_table = counterfactual_output_combined_table.to_latex(index=False, float_format=\"%.3f\")\n",
    "lines = latex_table.splitlines()\n",
    "lines.insert(4, r'\\midrule')\n",
    "lines.insert(5, r'\\multicolumn{2}{l}{\\textbf{No labor reallocation}} \\\\')\n",
    "lines.insert(6, r'\\midrule')\n",
    "# add a blank line\n",
    "lines.insert(10, r'\\midrule')\n",
    "lines.insert(11, r'\\multicolumn{2}{l}{\\textbf{Full labor reallocation}} \\\\')\n",
    "lines.insert(12, r'\\midrule')\n",
    "# lines.insert(3, r'\\midrule')\n",
    "# lines.insert(4, r'\\multicolumn{2}{c}{\\textbf{No labor reallocation}} \\\\')\n",
    "# lines.insert(5, r'\\midrule')\n",
    "latex_table = '\\n'.join(lines)\n",
    "\n",
    "with open(\"../../tables/cumulative_counterfactual_output_change.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use year-to-year changes in share parameters, predict the effect on sectoral output. Save results to compare to actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = .7\n",
    "\n",
    "# load elasticities\n",
    "theta = elasticity_bycode['mean_theta'].to_numpy()\n",
    "\n",
    "# initial guesses\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# set initial Omega\n",
    "temp_Omega = Omega.copy()\n",
    "\n",
    "# initialize counterfactual sectoral output\n",
    "counterfactual_output_change = pd.DataFrame({'Code': elasticity_bycode['Code']})\n",
    "\n",
    "# set starting point \n",
    "C_previous, Y_previous = eqm_solver(np.ones(len(alpha)), beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm) \n",
    "\n",
    "for i in range(1997, 2023):\n",
    "    # t = 0 calibration\n",
    "    temp_Omega = Omega_list[str(i)]\n",
    "    temp_alpha = alpha_list[str(i)].flatten()\n",
    "    temp_beta = beta_list[str(i)].flatten()\n",
    "    temp_L = L_list[str(i)].flatten()\n",
    "\n",
    "    # t = 1 change\n",
    "    Omega_counterfactual = temp_Omega.copy()\n",
    "    delta_Omega = np.exp(delta_logOmega_list[str(i+1)]).to_numpy()\n",
    "    Omega_counterfactual = Omega_counterfactual * delta_Omega\n",
    "    Omega_counterfactual = Omega_counterfactual / Omega_counterfactual.sum(axis=1)[:, np.newaxis] # normalize to 1\n",
    "\n",
    "    # corresponding beta_counterfactual, L_counterfactual\n",
    "    Y = Y_list[str(i)]\n",
    "    beta_counterfactual = Y@(np.eye(len(alpha)) - np.diag(alpha))@Omega_counterfactual\n",
    "    beta_counterfactual = beta_counterfactual / beta_counterfactual.sum()\n",
    "    L_counterfactual = alpha*(beta_counterfactual.T@np.linalg.inv(np.eye(len(alpha)) - (np.diag(1 - alpha)@Omega_counterfactual)))\n",
    "\n",
    "\n",
    "    # solve for C_0, Y_0, C_1, Y_1\n",
    "    C_0, Y_0 = eqm_solver(np.ones(len(alpha)), temp_beta, temp_Omega, temp_alpha, epsilon, theta, sigma, temp_L, P_norm, Y_norm)\n",
    "    C_1, Y_1 = eqm_solver(np.ones(len(alpha)), temp_beta, Omega_counterfactual, temp_alpha, epsilon, theta, sigma, temp_L, P_norm, Y_norm)\n",
    "    counterfactual_output_change[str(i+1)] = np.log(Y_1) - np.log(Y_0)\n",
    "\n",
    "counterfactual_output_change.to_csv(\"../../data/cleaned/structural/counterfactual_output_change_byyear.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect on sectoral shocks \n",
    "\n",
    "Using the counterfactual $\\tilde{\\Omega}_{2023}$ corresponding to share parameter shifts between 1997-2023, estimate the effect of severe shocks to individual sectors on GDP. Note that I list log-deviations from non-shocked GDP; since log output at the original point is 0, this doesn't matter, but it does for the counterfactual Omega, since that implies a different non-shocked GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eqm_solver_functions import *\n",
    "\n",
    "# set initial values for 1997\n",
    "year = '1997'\n",
    "Omega = Omega_list[year]\n",
    "alpha = alpha_list[year].flatten()\n",
    "beta = beta_list[year].flatten()\n",
    "L = L_list[year].flatten()\n",
    "\n",
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = .7\n",
    "\n",
    "# load elasticities\n",
    "theta = elasticity_bycode['mean_theta'].to_numpy()\n",
    "\n",
    "# get counterfactual Omega\n",
    "Omega_counterfactual = Omega.copy()\n",
    "Omega_counterfactual = Omega_counterfactual * np.exp(cumulative_delta_logOmega)\n",
    "Omega_counterfactual = Omega_counterfactual / Omega_counterfactual.sum(axis=1)[:, np.newaxis] # normalize rows to sum to 1\n",
    "\n",
    "# set initial guesses\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# recover reallocated L\n",
    "L_counterfactual = eqm_solver(np.ones(len(alpha)), beta, Omega_counterfactual, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, reallocate=True, return_L=True)\n",
    "\n",
    "# extreme shocks\n",
    "outlier = -np.log(2) # cut output by 75%\n",
    "extreme_shocks = []\n",
    "for i in range(len(alpha)): \n",
    "    shock = np.zeros(len(alpha))\n",
    "    shock[i] = outlier\n",
    "    extreme_shocks.append(shock)\n",
    "\n",
    "gdp_noshock_0, sectoral_output_noshock_0 = eqm_solver(np.ones(len(alpha)), beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm)\n",
    "gdp_dist_0, sectoral_output_0 = eqm_simulator(beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_dist_0 = np.log(gdp_dist_0) - np.log(gdp_noshock_0) # dataframe difference in gdp, shock - no shock\n",
    "gdp_noshock_1, sectoral_output_noshock_1 = eqm_solver(np.ones(len(alpha)), beta, Omega_counterfactual, alpha, epsilon, theta, sigma, L_counterfactual, P_norm, Y_norm)\n",
    "gdp_dist_1, sectoral_output_1= eqm_simulator(beta, Omega_counterfactual, alpha, epsilon, theta, sigma, L_counterfactual, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_dist_1 = np.log(gdp_dist_1) - np.log(gdp_noshock_1) # dataframe difference in gdp, shock - no shock\n",
    "gdp_diff = np.abs(gdp_dist_1) - np.abs(gdp_dist_0) # magnitude difference \n",
    "\n",
    "gdp_diff = pd.DataFrame({'2023':gdp_dist_1, '1997':gdp_dist_0, 'Difference': gdp_diff, 'Code': elasticity_bycode['Code']}) # dataframe with industry labels\n",
    "gdp_diff = gdp_diff.merge(code_desc_crosswalk, on='Code')\n",
    "gdp_diff = gdp_diff[['Industry Description', '2023', '1997', 'Difference']] \n",
    "gdp_diff['Industry Description'] = gdp_diff['Industry Description'].apply(lambda x: x[:50] + '...' if len(x) > 50 else x) # abridge industry descriptions at 50 characters\n",
    "\n",
    "diff_tab_decrease = gdp_diff.sort_values(by='Difference', ascending=True).head(3)\n",
    "diff_tab_increase = gdp_diff.sort_values(by='Difference', ascending=False).head(3)\n",
    "diff_tab = pd.concat([diff_tab_increase, diff_tab_decrease]) # stack dataframes\n",
    "diff_tab = diff_tab[['Industry Description', 'Difference']] # select columns\n",
    "diff_tab = diff_tab.rename(columns={'Difference': '$\\\\lvert \\\\Delta \\\\text{GDP}(\\\\tilde{\\\\Omega}_{2023}) \\\\rvert - \\\\lvert \\\\Delta \\\\text{GDP}(\\\\Omega_{1997}) \\\\rvert$'}) # rename columns\n",
    "\n",
    "# write table\n",
    "latex_table = diff_tab.to_latex(index=False, float_format=\"%.3f\", column_format='lccc')\n",
    "lines = latex_table.splitlines()\n",
    "lines.insert(4, r'\\midrule')\n",
    "lines.insert(5, r'\\multicolumn{2}{l}{\\textbf{Largest increases in GDP effect}} \\\\')\n",
    "lines.insert(6, r'\\midrule')\n",
    "# add a blank line\n",
    "lines.insert(10, r'\\midrule')\n",
    "lines.insert(11, r'\\multicolumn{2}{l}{\\textbf{Largest decreases in GDP effect}} \\\\')\n",
    "lines.insert(12, r'\\midrule')\n",
    "latex_table = '\\n'.join(lines)\n",
    "\n",
    "with open(\"../../tables/ind_sector_shock_1997_2023.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I do the same exercise, but the baseline year is 2023, and I generate $\\tilde{\\Omega}_{2033}$ to predict future share parameters based on linear trends (i.e. add the share parameter shifts from 2013-2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial values for 2023\n",
    "year = '2023'\n",
    "Omega = Omega_list[year]\n",
    "alpha = alpha_list[year].flatten()\n",
    "beta = beta_list[year].flatten()\n",
    "L = L_list[year].flatten()\n",
    "\n",
    "# exogenous elasticities\n",
    "epsilon = 0.6 # across VA and II; Alireza+ \n",
    "sigma = .7\n",
    "\n",
    "# load elasticities\n",
    "theta = elasticity_bycode['mean_theta'].to_numpy()\n",
    "\n",
    "# get predicted Omega\n",
    "Omega_predicted = Omega.copy()\n",
    "Omega_predicted = Omega_predicted * np.exp(delta_logOmega_predicted10)\n",
    "Omega_predicted = Omega_predicted / Omega_predicted.sum(axis=1)[:, np.newaxis] # normalize rows to sum to 1\n",
    "\n",
    "# set initial guesses\n",
    "Y_norm = beta@np.linalg.inv(np.eye(len(alpha)) - np.diag(1 - alpha)@Omega)\n",
    "P_norm = np.ones(len(alpha))\n",
    "\n",
    "# recover reallocated L\n",
    "L_counterfactual = eqm_solver(np.ones(len(alpha)), beta, Omega_predicted, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, reallocate=True, return_L=True)\n",
    "\n",
    "# extreme shocks\n",
    "outlier = -np.log(2) # cut output by 75%\n",
    "extreme_shocks = []\n",
    "for i in range(len(alpha)): \n",
    "    shock = np.zeros(len(alpha))\n",
    "    shock[i] = outlier\n",
    "    extreme_shocks.append(shock)\n",
    "\n",
    "gdp_noshock_0, sectoral_output_noshock_0 = eqm_solver(np.ones(len(alpha)), beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm)\n",
    "gdp_dist_0, sectoral_output_0 = eqm_simulator(beta, Omega, alpha, epsilon, theta, sigma, L, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_dist_0 = np.log(gdp_dist_0) - np.log(gdp_noshock_0) # dataframe difference in gdp, shock - no shock\n",
    "gdp_noshock_1, sectoral_output_noshock_1 = eqm_solver(np.ones(len(alpha)), beta, Omega_predicted, alpha, epsilon, theta, sigma, L_counterfactual, P_norm, Y_norm)\n",
    "gdp_dist_1, sectoral_output_1= eqm_simulator(beta, Omega_predicted, alpha, epsilon, theta, sigma, L_counterfactual, P_norm, Y_norm, extreme_shocks)\n",
    "gdp_dist_1 = np.log(gdp_dist_1) - np.log(gdp_noshock_1) # dataframe difference in gdp, shock - no shock\n",
    "gdp_diff = np.abs(gdp_dist_1) - np.abs(gdp_dist_0) # magnitude difference \n",
    "\n",
    "gdp_diff = pd.DataFrame({'2033':gdp_dist_1, '2023':gdp_dist_0, 'Difference': gdp_diff, 'Code': elasticity_bycode['Code']}) # dataframe with industry labels\n",
    "gdp_diff = gdp_diff.merge(code_desc_crosswalk, on='Code')\n",
    "gdp_diff = gdp_diff[['Industry Description', '2033', '2023', 'Difference']] \n",
    "gdp_diff['Industry Description'] = gdp_diff['Industry Description'].apply(lambda x: x[:50] + '...' if len(x) > 50 else x) # abridge industry descriptions at 50 characters\n",
    "\n",
    "diff_tab_decrease = gdp_diff.sort_values(by='Difference', ascending=True).head(3)\n",
    "diff_tab_increase = gdp_diff.sort_values(by='Difference', ascending=False).head(3)\n",
    "diff_tab = pd.concat([diff_tab_increase, diff_tab_decrease]) # stack dataframes\n",
    "diff_tab = diff_tab[['Industry Description', 'Difference']] # select columns\n",
    "diff_tab = diff_tab.rename(columns={'Difference': '$\\\\lvert \\\\Delta \\\\text{GDP}(\\\\tilde{\\\\Omega}_{2033}) \\\\rvert - \\\\lvert \\\\Delta \\\\text{GDP}(\\\\Omega_{2023}) \\\\rvert$'}) # rename columns\n",
    "\n",
    "# write table\n",
    "latex_table = diff_tab.to_latex(index=False, float_format=\"%.3f\", column_format='lccc')\n",
    "lines = latex_table.splitlines()\n",
    "lines.insert(4, r'\\midrule')\n",
    "lines.insert(5, r'\\multicolumn{2}{l}{\\textbf{Largest increases in GDP effect}} \\\\')\n",
    "lines.insert(6, r'\\midrule')\n",
    "# add a blank line\n",
    "lines.insert(10, r'\\midrule')\n",
    "lines.insert(11, r'\\multicolumn{2}{l}{\\textbf{Largest decreases in GDP effect}} \\\\')\n",
    "lines.insert(12, r'\\midrule')\n",
    "latex_table = '\\n'.join(lines)\n",
    "\n",
    "with open(\"../../tables/ind_sector_shock_2023_2033.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MacM3Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
